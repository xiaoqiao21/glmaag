?sample
setwd("F:/kaggle/housing price/data")
setwd("~/")
library(h2o)
h2o.xgboost.available()
h2o.init()
h2o.xgboost.available()
install.packages('h2o')
library(h2o)
h2o.init()
920+84+50+2497.22+103.24-1196.99-103.24+627.54+27.75+191.98
install.packages('lime')
library(lime)
88*4*9*.22
749/4225
160+2500*.02+1250*.04
120*800000
262330+1049945+211311+3913726+60000000+2998098+68952+1499360+3958452+4003683
4003683*.25+3958452*.1+1499360*.1+2998098*.3+60000000*.1+3913726*.3+1049945*.02
library(dlstats)
aa=cran_stats('JumpTest')
sum(aa$downloads)
200+500+900+2500
200+500+1500
106,780.23+11,225.83+4062.48+501.56+43.05+3132.64+1111.89
106780.23+11225.83+4062.48+501.56+43.05+3132.64+1111.89
7600*12
3854*27
library(xgboost)
install.packages('xgboost')
library(xgboost)
data(agaricus.train, package = "xgboost")
dtrain <- xgb.DMatrix(agaricus.train$data,
label = agaricus.train$label)
cv_folds <- KFold(agaricus.train$label, nfolds = 5,
stratified = TRUE, seed = 0)
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0)
list(Score = cv$dt[, max(test.auc.mean)],
Pred = cv$pred)
}
library(rBayesianOptimization)
install.packages('rBayesianOptimization')
library(rBayesianOptimization)
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
cv_folds <- KFold(agaricus.train$label, nfolds = 5,
stratified = TRUE, seed = 0)
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0)
list(Score = cv$dt[, max(test.auc.mean)],
Pred = cv$pred)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
?xgb.cv
data(agaricus.train, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
cv <- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = list("rmse","auc"),
max_depth = 3, eta = 1, objective = "binary:logistic")
cv
cv$dt
data(agaricus.train, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
cv <- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = 'auc',
max_depth = 3, eta = 1, objective = "binary:logistic")
cv
cv$niter
cv$folds
cv$evaluation_log
cv$evaluation_log[,max(test_auc_mean)]
cv$pred
data(agaricus.train, package='xgboost')
dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)
cv <- xgb.cv(data = dtrain, nrounds = 3, nthread = 2, nfold = 5, metrics = 'auc',
max_depth = 3, eta = 1, objective = "binary:logistic", prediction = T)
cv$pred
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0,
prediction = T)
list(Score = cv$evaluation_log[,max(test_auc_mean)],
Pred = cv$pred)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
xgb_cv_bayes <- function(max.depth, min_child_weight, subsample) {
cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
max_depth = max.depth,
min_child_weight = min_child_weight,
subsample = subsample, colsample_bytree = 0.3,
lambda = 1, alpha = 0,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain, nround = 100,
folds = cv_folds, prediction = TRUE, showsd = TRUE,
early.stop.round = 5, maximize = TRUE, verbose = 0)
list(Score = cv$evaluation_log[,max(test_auc_mean)],
Pred = cv$pred)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max.depth = c(2L, 6L),
min_child_weight = c(1L, 10L),
subsample = c(0.5, 0.8)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
library(h2o)
h2o.init()
agaricus.train
train <- cbind(agaricus.train$label, agaricus.train$data)
head(train)
dim(train)
train[1:5,]
agaricus.train$label
train <- data.frame(label = agaricus.train$label, data = agaricus.train$data)
train <- data.frame(label = agaricus.train$label, data = as.data.frame(agaricus.train$data))
agaricus.train$data
as.matrix(agaricus.train$data)
train <- data.frame(label = agaricus.train$label, data = as.data.frame(as.matrix(agaricus.train$data)))
train
train <- as.h2o(train)
train
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
?h2o.randomForest
irisPath = system.file("extdata", "iris.csv", package="h2o")
iris.hex = h2o.uploadFile(localH2O, path = irisPath, key = "iris.hex")
data(iris)
iris
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfhex
rfHex
train <- data.frame(label = as.factor(agaricus.train$label), data = as.data.frame(as.matrix(agaricus.train$data)))
train <- as.h2o(train)
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex
h2o.performance(rfHex)
rfHex$mode
rfHex@model
h2o.performance(rfHex)$cross_validation_metrics
h2o.performance(rfHex)@cross_validation_metrics
h2o.auc(rfHex)
rfHex
slot(rfHex@model)
unslot(rfHex@model)
rfHex
data(iris)
library(data.table)
setDT(iris)
iris
iris_tr <- iris[, Species %in% c('setosa', 'virginica')]
iris_tr
iris_tr <- iris[Species %in% c('setosa', 'virginica')]
iris_tr
train <- as.h2o(iris_tr)
train
rfHex <- h2o.randomForest(y = 'label',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex <- h2o.randomForest(y = 'Species',
ntrees = 100,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex
rfHex <- h2o.randomForest(y = 'Species',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex
prostate_path <- system.file("extdata", "prostate.csv", package = "h2o")
prostate <- h2o.uploadFile(prostate_path)
prostate
model <- h2o.gbm(x = 3:9, y = 2, training_frame = prostate, distribution = "bernoulli")
model <- h2o.gbm(x = 3:9, y = 2, training_frame = prostate, distribution = "bernoulli")
prostate[,2] <- as.factor(prostate[,2])
model <- h2o.gbm(x = 3:9, y = 2, training_frame = prostate, distribution = "bernoulli")
perf <- h2o.performance(model, prostate)
h2o.auc(perf)
prostate
rfHex <- h2o.randomForest(y = 'CAPSULE',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = train)
rfHex <- h2o.randomForest(y = 'CAPSULE',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = prostate)
rfHex
h2o.auc(rfHex)
h2o.performance(rfHex)
?h2o.performance
h2o.performance(rfHex, train = T)
h2o.performance(rfHex, valid = T)
h2o.performance(rfHex)
h2o.auc(h2o.performance(rfHex))
h2o.auc(rfHex)
h2o.predict(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)
h2o.cross_validation_models(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)
h2o.cross_validation_redictions(rfHex)
h2o.cross_validation_predictions(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)
?h2o.cross_validation_holdout_predictions
h2o.cross_validation_holdout_predictions(rfHex)[,p1]
h2o.cross_validation_holdout_predictions(rfHex)$p1
rfHex <- h2o.randomForest(y = 'CAPSULE',
ntrees = 1000,
max_depth = 30,
keep_cross_validation_predictions = T,
nfolds = 5,
seed = 42,
nbins_cats = 1115, ## allow it to fit store ID
training_frame = prostate)
h2o.auc(rfHex)
h2o.cross_validation_holdout_predictions(rfHex)$p1
?h2o.randomForest
rf_cv_prediction <- function(ntrees, max_depth, min_rows, learn_rate, sample_rate, col_sample_rate, col_sample_rate_per_tree, nbins_cats) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntress = ntrees,
max_depth = max_depth,
min_rows = min_rows,
learn_rate = learn_rate,
sample_rate = sample_rate,
col_sample_rate = col_sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
nbins_cats = nbins_cats,
keep_cross_validation_predictions = T,
keep_cross_validation_predictions = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
rf_cv_bayes <- function(ntrees, max_depth, min_rows, learn_rate, sample_rate, col_sample_rate, col_sample_rate_per_tree, nbins_cats) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntress = ntrees,
max_depth = max_depth,
min_rows = min_rows,
learn_rate = learn_rate,
sample_rate = sample_rate,
col_sample_rate = col_sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
nbins_cats = nbins_cats,
keep_cross_validation_predictions = T,
keep_cross_validation_predictions = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
learn_rate = c(.001, .01),
sample_rate = c(.1, 1),
col_sample_rate = c(.1, 1),
col_sample_rate_per_tree = c(.1, 1)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
rf_cv_bayes <- function(ntrees, max_depth, min_rows, learn_rate, sample_rate, col_sample_rate, col_sample_rate_per_tree) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntrees = ntrees,
max_depth = max_depth,
min_rows = min_rows,
learn_rate = learn_rate,
sample_rate = sample_rate,
col_sample_rate = col_sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
keep_cross_validation_predictions = T,
keep_cross_validation_models = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
learn_rate = c(.001, .01),
sample_rate = c(.1, 1),
col_sample_rate = c(.1, 1),
col_sample_rate_per_tree = c(.1, 1)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
rf_cv_bayes <- function(ntrees, max_depth, min_rows, sample_rate, col_sample_rate_per_tree, col_sample_rate_change_per_level, mtries) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntrees = ntrees,
max_depth = max_depth,
min_rows = min_rows,
sample_rate = sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
col_sample_rate_change_per_level = col_sample_rate_change_per_level,
mtries = mtries,
keep_cross_validation_predictions = T,
keep_cross_validation_models = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
sample_rate = c(.1, 1),
mtries = c(10L, 20L)
col_sample_rate_per_tree = c(.1, 1),
col_sample_rate_change_per_level = c(.1, 2)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
sample_rate = c(.1, 1),
mtries = c(10L, 20L),
col_sample_rate_per_tree = c(.1, 1),
col_sample_rate_change_per_level = c(.1, 2)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
rf_cv_bayes <- function(ntrees, max_depth, min_rows, sample_rate, col_sample_rate_per_tree, col_sample_rate_change_per_level) {
cv <- h2o.randomForest(y = 'CAPSULE',
ntrees = ntrees,
max_depth = max_depth,
min_rows = min_rows,
sample_rate = sample_rate,
col_sample_rate_per_tree = col_sample_rate_per_tree,
col_sample_rate_change_per_level = col_sample_rate_change_per_level,
keep_cross_validation_predictions = T,
keep_cross_validation_models = F,
nfolds = 5,
seed = 42,
training_frame = prostate)
list(Score = h2o.auc(cv),
Pred = h2o.cross_validation_holdout_predictions(cv)$p1)
}
PT_Res <- BayesianOptimization(rf_cv_bayes, bounds = list(ntrees = c(100L, 2000L),
max_depth = c(1L, 20L),
min_rows = c(1L, 100L),
sample_rate = c(.1, 1),
col_sample_rate_per_tree = c(.1, 1),
col_sample_rate_change_per_level = c(.1, 2)),
init_grid_dt = NULL, init_points = 10, n_iter = 20,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
library(h2o)
?h2o.deeplearning
?h2o.randomForest
29.78-2.99
install.packages('ROI')
library(ROC)
library(ROI)
150/4
6.5*3+16
6.5*3+48/3
2^12-1
install.packages('RcppArmadillo')
install.packages('installr')
library(installr)
updateR(copy_packages = F, keep_old_packages = F)
?system
library(glmaag)
install.packages('glmaag')
install.packages("glmaag")
remove.packages('glmaag')
library(glmaag)
install.packages('glmaag')
library(glmaag)
?cv_glmaag
y <- sampledata$Y_Gau
data(sampledata)
data(L0)
y <- sampledata$Y_Gau
x <- sampledata[, -(1:3)]
mod <- cv_glmaag(y, x, L0)
system.time(mod <- cv_glmaag(y, x, L0))
?ss_glmaag
system.time(mod <- ss_glmaag(y, x, L0))
?system.time
dim）
dim(mod)
dim(x)
install.packages('chron')
library(chron)
lt <- as.POSIXlt("2010-10-18 21:46:53")
lt
tt1 <- times(format(lt, "%H:%M:%S")); tt1
times(lt)
?time
class(tt1)
trunc(tt1, 'hours')
truc(lt, 'hours')
trunc(lt, 'hours')
as.Date()
?as.Date
as.Date(lt, '%H-%M-%S')
17.71+10.12+5.06+11.28+14.67
10+26.05+23.12+45.9
58.84+105.07+15.51
?install.packages
library(devtools)
library(rmarkdown)
library(knitr)
setwd("F:/Rpack/glmaag")
file.edit('DESCRIPTION')
load_all()
document()
build_vignettes()
build()
check(manual = T)
?check
check(manual = T, remote = T, run_dont_test = T)
setwd("F:/Rpack/glmaag/data")
load('sampledata.rda')
rm(list=ls())
setwd("F:/Rpack/glmaag")
load_all()
document()
build_vignettes()
build()
check(manual = T, remote = T, run_dont_test = T)
load_all()
load_all()
load_all()
()
document()
build_vignettes()
build()
build_vignettes()
check(manual = T, remote = T, run_dont_test = T)
?seq_len
load_all()
document()
build_vignettes()
build()
check(manual = T, remote = T, run_dont_test = T)
